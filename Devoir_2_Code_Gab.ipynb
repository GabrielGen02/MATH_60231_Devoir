{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628c1896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "pgpass file created at C:\\Users\\Gabri\\AppData\\Roaming\\postgresql\\pgpass.conf\n",
      "Created .pgpass file successfully.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# connexion a wrds \n",
    "import wrds\n",
    "db = wrds.Connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "754c497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Connexion WRDS...\n",
      "WRDS recommends setting up a .pgpass file.\n",
      "pgpass file created at C:\\Users\\Gabri\\AppData\\Roaming\\postgresql\\pgpass.conf\n",
      "Created .pgpass file successfully.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n",
      ">>> Connexion WRDS OK.\n",
      "\n",
      ">>> 13488 tickers ont au moins un EPS disponible.\n",
      ">>> Échantillon final : 50 tickers sélectionnés.\n",
      ">>> Prix téléchargés.\n",
      "\n",
      ">>> EPS téléchargés.\n",
      "\n",
      ">>> Jointure CRSP + EPS effectuée.\n",
      "\n",
      ">>> Trailing EPS & P/E calculés.\n",
      "\n",
      "              date    prc  permno ticker   gvkey    datadate  epspxq  eps_ttm  \\\n",
      "2298858 2021-06-30  26.06   11628   CPBI  043152  2021-06-30    <NA>      NaN   \n",
      "2298875 2021-07-01  26.17   11628   CPBI  043152  2021-06-30    <NA>      NaN   \n",
      "2298892 2021-07-02  25.81   11628   CPBI  043152  2021-06-30    <NA>      NaN   \n",
      "2298909 2021-07-06  25.29   11628   CPBI  043152  2021-06-30    <NA>      NaN   \n",
      "2298926 2021-07-07  24.85   11628   CPBI  043152  2021-06-30    <NA>      NaN   \n",
      "\n",
      "         trailing_pe  \n",
      "2298858         <NA>  \n",
      "2298875         <NA>  \n",
      "2298892         <NA>  \n",
      "2298909         <NA>  \n",
      "2298926         <NA>  \n",
      ">>> Pipeline terminé, CSV sauvegardé.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import wrds\n",
    "import pandas as pd\n",
    "import numpy as np  # pour np.random.choice\n",
    "\n",
    "np.random.seed(42)  # pour reproductibilité\n",
    "\n",
    "# --- 1️ Connexion WRDS ---\n",
    "def connect_wrds():\n",
    "    print(\">>> Connexion WRDS...\")\n",
    "    db = wrds.Connection()\n",
    "    print(\">>> Connexion WRDS OK.\\n\")\n",
    "    return db\n",
    "\n",
    "# --- 2️ Échantillon aléatoire de 50 entreprises S&P500 ---\n",
    "def get_sp500_sample(df, db):\n",
    "    \"\"\"\n",
    "    Retourne un échantillon de 50 tickers du S&P500 avec EPS disponibles.\n",
    "    df : dataframe initial CRSP (permno + ticker)\n",
    "    db : connexion WRDS\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    np.random.seed(42)  # reproductibilité\n",
    "\n",
    "    # Tickres uniques\n",
    "    unique_tickers = df[\"ticker\"].dropna().unique()\n",
    "\n",
    "    # Vérifier quels tickers ont des données EPS\n",
    "    valid_tickers = []\n",
    "    for ticker in unique_tickers:\n",
    "        query = f\"\"\"\n",
    "            SELECT 1\n",
    "            FROM comp.fundq\n",
    "            WHERE tic = '{ticker}'\n",
    "            LIMIT 1\n",
    "        \"\"\"\n",
    "        result = db.raw_sql(query)\n",
    "        if not result.empty:\n",
    "            valid_tickers.append(ticker)\n",
    "\n",
    "    print(f\">>> {len(valid_tickers)} tickers ont au moins un EPS disponible.\")\n",
    "\n",
    "    # Échantillon aléatoire de 50 tickers\n",
    "    if len(valid_tickers) < 50:\n",
    "        print(\"Attention : moins de 50 tickers avec EPS, on prend tout ce qui est disponible.\")\n",
    "        sample_tickers = valid_tickers\n",
    "    else:\n",
    "        sample_tickers = np.random.choice(valid_tickers, size=50, replace=False)\n",
    "\n",
    "    # Construire le dataframe final (ticker + permno)\n",
    "    sample = df[df[\"ticker\"].isin(sample_tickers)][[\"permno\", \"ticker\"]].drop_duplicates()\n",
    "\n",
    "    print(f\">>> Échantillon final : {len(sample['ticker'].unique())} tickers sélectionnés.\")\n",
    "    return sample\n",
    "\n",
    "\n",
    "# --- 3️ Extraction des prix CRSP ---\n",
    "def get_prices(db, sample):\n",
    "    permno_list = \"', '\".join(sample['permno'].astype(str).unique())\n",
    "    query = f\"\"\"\n",
    "        SELECT date, prc, permno\n",
    "        FROM crsp.dsf\n",
    "        WHERE permno IN ('{permno_list}')\n",
    "        ORDER BY permno, date\n",
    "    \"\"\"\n",
    "    prices = db.raw_sql(query)\n",
    "    print(\">>> Prix téléchargés.\\n\")\n",
    "    return prices\n",
    "\n",
    "# --- 4️ Extraction des EPS Compustat ---\n",
    "def get_eps(db, sample):\n",
    "    tic_list = \"', '\".join(sample['ticker'].unique())\n",
    "    query = f\"\"\"\n",
    "        SELECT gvkey, tic AS ticker, datadate, epspxq\n",
    "        FROM comp.fundq\n",
    "        WHERE tic IN ('{tic_list}')\n",
    "        ORDER BY gvkey, datadate\n",
    "    \"\"\"\n",
    "    eps = db.raw_sql(query)\n",
    "    print(\">>> EPS téléchargés.\\n\")\n",
    "    return eps\n",
    "\n",
    "# --- 5️ Jointure CRSP ↔ EPS via ticker ---\n",
    "def merge_prices_eps(prices, eps, sample):\n",
    "    merged = prices.merge(sample, on='permno', how='left')\n",
    "    merged = merged.merge(eps, on='ticker', how='left')\n",
    "    \n",
    "    # Filtrer EPS publié avant la date du prix\n",
    "    merged = merged[merged['datadate'] <= merged['date']]\n",
    "    print(\">>> Jointure CRSP + EPS effectuée.\\n\")\n",
    "    return merged\n",
    "\n",
    "# --- 6️ Calcul Trailing P/E ---\n",
    "def compute_trailing_PE(merged):\n",
    "    merged = merged.sort_values(['permno', 'date'])\n",
    "    merged['eps_ttm'] = merged.groupby('permno')['epspxq'].rolling(4).sum().reset_index(level=0, drop=True)\n",
    "    merged['eps_ttm'] = merged.groupby('permno')['eps_ttm'].ffill()\n",
    "    merged['trailing_pe'] = merged['prc'] / merged['eps_ttm']\n",
    "    print(\">>> Trailing EPS & P/E calculés.\\n\")\n",
    "    return merged\n",
    "\n",
    "# --- 7️ Pipeline complet ---\n",
    "def run_pipeline():\n",
    "    db = connect_wrds()\n",
    "\n",
    "    # Charger la liste S&P500 depuis CRSP\n",
    "    df = db.raw_sql(\"\"\"\n",
    "        SELECT permno, ticker\n",
    "        FROM crsp.msenames\n",
    "        WHERE shrcd IN (10,11)\n",
    "        AND exchcd IN (1,2,3)\n",
    "    \"\"\")\n",
    "\n",
    "    sample = get_sp500_sample(df, db)\n",
    "    prices = get_prices(db, sample)\n",
    "    eps = get_eps(db, sample)\n",
    "\n",
    "    merged = merge_prices_eps(prices, eps, sample)\n",
    "    final = compute_trailing_PE(merged)\n",
    "    \n",
    "    # Filtrer pour les dates après 2003\n",
    "    final['date'] = pd.to_datetime(final['date'])\n",
    "    final = final[final['date'] > \"2003-01-01\"]\n",
    "\n",
    "    print(final.head())\n",
    "    return final\n",
    "\n",
    "# --- 8️ Exécution ---\n",
    "if __name__ == \"__main__\":\n",
    "    final_df = run_pipeline()\n",
    "    final_df.to_csv(\"SP500_trailing_PE.csv\", index=False)\n",
    "    print(\">>> Pipeline terminé, CSV sauvegardé.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd0976",
   "metadata": {},
   "source": [
    "### Nettoyage et préparation des données\n",
    "\n",
    "1. **Renommer les colonnes** pour plus de clarté : `prc → prix`, `epspxq → EPS`, `eps_ttm → EPS_MOBILE`, `trailing_pe → Prix_bene`.\n",
    "\n",
    "2. **Trier et convertir les dates** par entreprise (`permno`) et date, supprimer colonnes temporaires inutiles.\n",
    "\n",
    "3. **Filtrer les anciennes données** : garder uniquement les EPS à partir de 2003.\n",
    "\n",
    "4. **Remplir les valeurs manquantes** (NaN) par forward/backfill par entreprise et supprimer les tickers avec >30% de NaN.\n",
    "\n",
    "5. **Convertir en float** les colonnes `prix` et `EPS`.\n",
    "\n",
    "6. **Supprimer doublons** par ticker et date, garder la dernière ligne.\n",
    "\n",
    "7. **Préparer les données trimestrielles** : garder `permno`, `datadate`, `epspxq`, une seule ligne par trimestre et par entreprise.\n",
    "\n",
    "8. **Calculer le BPA glissant sur 4 trimestres** (`EPS_MA4`) en moyenne des 4 derniers trimestres pour chaque entreprise.\n",
    "\n",
    "9. **Fusionner les données journalières et trimestrielles** : associer à chaque ligne quotidienne le BPA glissant du dernier trimestre disponible (`merge_asof` avec `by='permno'`).\n",
    "\n",
    "10. **Calculer les ratios prix/BPA** :  \n",
    "    - Historique : `prix / EPS`  \n",
    "    - Glissant : `prix / EPS_MA4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types de données :\n",
      " date        datetime64[ns]\n",
      "prix               Float64\n",
      "permno               Int64\n",
      "ticker      string[python]\n",
      "gvkey       string[python]\n",
      "datadate    datetime64[ns]\n",
      "EPS                Float64\n",
      "dtype: object\n",
      "Date minimale : 2003-01-31 00:00:00\n",
      "Date maximale : 2024-12-31 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>prix</th>\n",
       "      <th>permno</th>\n",
       "      <th>ticker</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>datadate</th>\n",
       "      <th>EPS</th>\n",
       "      <th>EPS_MA4</th>\n",
       "      <th>ratio_pb_h</th>\n",
       "      <th>ratio_pb_MA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52104</th>\n",
       "      <td>2024-12-24</td>\n",
       "      <td>135.03</td>\n",
       "      <td>90175</td>\n",
       "      <td>XPO</td>\n",
       "      <td>144998</td>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>166.703704</td>\n",
       "      <td>169.849057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52105</th>\n",
       "      <td>2024-12-26</td>\n",
       "      <td>133.17</td>\n",
       "      <td>90175</td>\n",
       "      <td>XPO</td>\n",
       "      <td>144998</td>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>164.407407</td>\n",
       "      <td>167.509434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52106</th>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>132.25999</td>\n",
       "      <td>90175</td>\n",
       "      <td>XPO</td>\n",
       "      <td>144998</td>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>163.283938</td>\n",
       "      <td>166.364767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52107</th>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>132.59</td>\n",
       "      <td>90175</td>\n",
       "      <td>XPO</td>\n",
       "      <td>144998</td>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>163.691358</td>\n",
       "      <td>166.779874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52108</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>131.14999</td>\n",
       "      <td>90175</td>\n",
       "      <td>XPO</td>\n",
       "      <td>144998</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8325</td>\n",
       "      <td>201.769215</td>\n",
       "      <td>157.537526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date       prix  permno ticker   gvkey   datadate   EPS  EPS_MA4  \\\n",
       "52104 2024-12-24     135.03   90175    XPO  144998 2024-09-30  0.81   0.7950   \n",
       "52105 2024-12-26     133.17   90175    XPO  144998 2024-09-30  0.81   0.7950   \n",
       "52106 2024-12-27  132.25999   90175    XPO  144998 2024-09-30  0.81   0.7950   \n",
       "52107 2024-12-30     132.59   90175    XPO  144998 2024-09-30  0.81   0.7950   \n",
       "52108 2024-12-31  131.14999   90175    XPO  144998 2024-12-31  0.65   0.8325   \n",
       "\n",
       "       ratio_pb_h  ratio_pb_MA  \n",
       "52104  166.703704   169.849057  \n",
       "52105  164.407407   167.509434  \n",
       "52106  163.283938   166.364767  \n",
       "52107  163.691358   166.779874  \n",
       "52108  201.769215   157.537526  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nettoyage et préparations des données \n",
    "def nettoyage_df(df):\n",
    "    # Renommer les colonnes proprement\n",
    "    df = df.rename(columns={\n",
    "        \"prc\": \"prix\",\n",
    "        \"epspxq\": \"EPS\",\n",
    "        \"eps_ttm\": \"EPS_MOBILE\",\n",
    "        \"trailing_pe\": \"Prix_bene\"\n",
    "    })\n",
    "    \n",
    "    # Trier les données par entreprise et par date\n",
    "    df = df.sort_values(['permno', 'date'])\n",
    "    \n",
    "    # Supprimer les colonnes temporaires\n",
    "    df = df.drop(columns=['EPS_MOBILE', 'Prix_bene'])\n",
    "    \n",
    "    # Convertir les dates\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['datadate'] = pd.to_datetime(df['datadate'])\n",
    "    \n",
    "    # Filtrer les EPS trop anciens : garder EPS >= 2003\n",
    "    df = df[df['datadate'] >= \"2003-01-01\"]\n",
    "    \n",
    "    # Remplir les NaN dans EPS par forward-fill + backfill par permno\n",
    "    df['EPS'] = df.groupby('permno')['EPS'].transform(lambda x: x.ffill().bfill())\n",
    "    \n",
    "    # Garder uniquement les tickers avec moins de 30% de NaN sur EPS\n",
    "    nan_eps_pct = df.groupby('ticker')['EPS'].apply(lambda x: x.isna().mean())\n",
    "    tickers_valides = nan_eps_pct[nan_eps_pct <= 0.30].index\n",
    "    df = df[df['ticker'].isin(tickers_valides)]\n",
    "    \n",
    "    # Convertir les colonnes en float\n",
    "    float_cols = ['prix', 'EPS']\n",
    "    for col in float_cols:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    \n",
    "    # Vérifier le type des colonnes\n",
    "    print(\"Types de données :\\n\", df.dtypes)\n",
    "    \n",
    "    # Trier par ticker et date pour cohérence\n",
    "    df = df.sort_values(['permno', 'date'], ascending=True).reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['ticker', 'date'], keep='last')\n",
    "    \n",
    "    # défintion de df quarter \n",
    "    df_quarter = final_df[['permno', 'datadate', 'epspxq']].dropna()\n",
    "\n",
    "    # Convertir datadate en datetime si ce n’est pas déjà le cas\n",
    "    df_quarter['datadate'] = pd.to_datetime(df_quarter['datadate'])\n",
    "\n",
    "    # Garder une seule ligne par permno et trimestre\n",
    "    df_quarter = df_quarter.sort_values(['permno', 'datadate'])\n",
    "    df_quarter = df_quarter.drop_duplicates(subset=['permno', 'datadate'], keep='first')\n",
    "\n",
    "    #  Calcul EPS mobile 4 trimestres\n",
    "    df_quarter['EPS_MA4'] = (\n",
    "        df_quarter\n",
    "        .groupby('permno')['epspxq']\n",
    "        .rolling(window=4, min_periods=4)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Trier globalement par date pour deux df\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    df_quarter = df_quarter.sort_values('datadate').reset_index(drop=True)\n",
    "\n",
    "    #  Merge des deux df\n",
    "    df = pd.merge_asof(\n",
    "        df,\n",
    "        df_quarter[['permno', 'datadate', 'EPS_MA4']],\n",
    "        left_on='date',\n",
    "        right_on='datadate',\n",
    "        by='permno',\n",
    "        direction='backward'\n",
    "    )\n",
    "\n",
    "    # Vérifier les dates min/max\n",
    "    print(\"Date minimale :\", df['date'].min())\n",
    "    print(\"Date maximale :\", df['date'].max())\n",
    "\n",
    "    # Supprimer les lignes avec NaN\n",
    "    df = df.dropna(subset=[\"EPS_MA4\"])\n",
    "\n",
    "    # calcule du ratio p/b historique et glissant \n",
    "    df[\"ratio_pb_h\"] = df[\"prix\"] / df[\"EPS\"] # histo \n",
    "    df[\"ratio_pb_MA\"] = df[\"prix\"] / df[\"EPS_MA4\"] # glissant\n",
    "\n",
    "    # présence de NaN par colone \n",
    "    df.isna().sum()\n",
    "\n",
    "    # Supprimer les colonnes doublés \n",
    "    df = df.drop(columns=['datadate_y'])\n",
    "    \n",
    "    # Renommer la colone doublé \n",
    "    df = df.rename(columns={\n",
    "        \"datadate_x\": \"datadate\"\n",
    "    })\n",
    "\n",
    "    # trie final par ticker et date \n",
    "    df = df.sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# résultats \n",
    "df = nettoyage_df(final_df)\n",
    "df.tail() # visualisation des résultats "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
